# Optimizers_JAX

Toy implementations of the following deep learning optimization algorithms from scratch in [JAX](https://opensource.google/projects/jax) - 

- Stochastic Gradient Descent
- Momentum
- Adagrad
- RMSProp
- Adam
